{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting Program\n",
      "INFO:Application_Logs:Starting Program\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress : 10%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Links for log files\n",
      "INFO:Application_Logs:Generating Links for log files\n",
      "Downloading and Extracting Log files\n",
      "INFO:Application_Logs:Downloading and Extracting Log files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress : 30%\n",
      "It may take some time\n",
      "Progress : 85%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Completed\n",
      "INFO:Application_Logs:Downloading Completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 90%\r",
      "Progress : 100%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting Cleaning of Data\n",
      "INFO:Application_Logs:Starting Cleaning of Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 0%\r",
      "Progress : 1%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning log20030101.csv\n",
      "INFO:Application_Logs:Cleaning log20030101.csv\n",
      "Cleaning log20030201.csv\n",
      "INFO:Application_Logs:Cleaning log20030201.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 3%\r",
      "Progress : 5%\r",
      "Progress : 7%\r",
      "Progress : 10%\r",
      "Progress : 16%\r",
      "Progress : 22%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning log20030301.csv\n",
      "INFO:Application_Logs:Cleaning log20030301.csv\n",
      "Cleaning log20030401.csv\n",
      "INFO:Application_Logs:Cleaning log20030401.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 28%\r",
      "Progress : 34%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning log20030501.csv\n",
      "INFO:Application_Logs:Cleaning log20030501.csv\n",
      "Cleaning log20030601.csv\n",
      "INFO:Application_Logs:Cleaning log20030601.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 40%\r",
      "Progress : 46%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning log20030701.csv\n",
      "INFO:Application_Logs:Cleaning log20030701.csv\n",
      "Cleaning log20030801.csv\n",
      "INFO:Application_Logs:Cleaning log20030801.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 52%\r",
      "Progress : 58%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning log20030901.csv\n",
      "INFO:Application_Logs:Cleaning log20030901.csv\n",
      "Cleaning log20031001.csv\n",
      "INFO:Application_Logs:Cleaning log20031001.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 64%\r",
      "Progress : 70%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning log20031101.csv\n",
      "INFO:Application_Logs:Cleaning log20031101.csv\n",
      "Cleaning log20031201.csv\n",
      "INFO:Application_Logs:Cleaning log20031201.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 76%\r",
      "Progress : 82%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning README.txt\n",
      "INFO:Application_Logs:Cleaning README.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Cleaned and Saved\n",
      "\r",
      "Progress : 100%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing Data\n",
      "INFO:Application_Logs:Summarizing Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 0%\r",
      "Progress : 1%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing for Cleaned_log20030101.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20030101.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 4%\r",
      "Progress : 24%\r",
      "Progress : 30%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing for Cleaned_log20030201.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20030201.csv\n",
      "Summarizing for Cleaned_log20030301.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20030301.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 36%\r",
      "Progress : 42%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing for Cleaned_log20030401.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20030401.csv\n",
      "Summarizing for Cleaned_log20030501.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20030501.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 48%\r",
      "Progress : 54%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing for Cleaned_log20030601.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20030601.csv\n",
      "Summarizing for Cleaned_log20030701.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20030701.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 60%\r",
      "Progress : 66%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing for Cleaned_log20030801.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20030801.csv\n",
      "Summarizing for Cleaned_log20030901.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20030901.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 72%\r",
      "Progress : 78%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing for Cleaned_log20031001.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20031001.csv\n",
      "Summarizing for Cleaned_log20031101.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20031101.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 84%\r",
      "Progress : 90%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing for Cleaned_log20031201.csv\n",
      "INFO:Application_Logs:Summarizing for Cleaned_log20031201.csv\n",
      "Data Summarized and Saved\n",
      "INFO:Application_Logs:Data Summarized and Saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 96%\r",
      "Progress : 100%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Data\n",
      "INFO:Application_Logs:Compiling Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress : 90%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Problem2_Compiled Files Zipped\n",
      "INFO:Application_Logs:Problem2_Compiled Files Zipped\n",
      "Uploading to Amazon s3\n",
      "INFO:Application_Logs:Uploading to Amazon s3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress : 92%.........."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Uploaded to S3\n",
      "INFO:Application_Logs:Files Uploaded to S3\n",
      "Compiled and uploaded to Amazon s3\n",
      "INFO:Application_Logs:Compiled and uploaded to Amazon s3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 100%"
     ]
    }
   ],
   "source": [
    "#Generate links foreach quarter\n",
    "def qtr1_url(year):\n",
    "    for i in range(1,4):\n",
    "       url.append(\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\" + str(year) + \"/\"+\"Qtr1/\"+\"log\"+str(year)+\"0\"+str(i)+\"01.zip\")\n",
    "       \n",
    "def qtr2_url(year):\n",
    "    for i in range(4,7):\n",
    "       url.append(\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\" + str(year) + \"/\"+\"Qtr2/\"+\"log\"+str(year)+\"0\"+str(i)+\"01.zip\")\n",
    "def qtr3_url(year):\n",
    "    for i in range(7,10):\n",
    "       url.append(\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\" + str(year) + \"/\"+\"Qtr3/\"+\"log\"+str(year)+\"0\"+str(i)+\"01.zip\")\n",
    "      \n",
    "def qtr4_url(year):\n",
    "    for i in range(10,13):\n",
    "       url.append(\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\" + str(year) + \"/\"+\"Qtr4/\"+\"log\"+str(year)+str(i)+\"01.zip\")\n",
    " \n",
    "     \n",
    "        \n",
    " \n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    create_directory(\"Files2\")\n",
    "    loglevel = logging.INFO            # DEBUG, CRITICAL, WARNING, ERROR\n",
    "    logger = logging.getLogger(\"Application_Logs\")\n",
    "    logger2 = logging.getLogger(\"Application_Logs_Stream\")\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger.setLevel(logging.INFO)\n",
    "#         Logfile handler\n",
    "        handler = logging.FileHandler('Files2/logs.log')\n",
    "        handler2 = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.addHandler(handler2)\n",
    "        logger.setLevel(loglevel)\n",
    "        logger.handler_set = True\n",
    "#       Stream Handler\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger2.setLevel(logging.INFO)\n",
    "        handler2 = logging.StreamHandler()\n",
    "        handler2.setFormatter(formatter)\n",
    "        logger2.addHandler(handler2)\n",
    "        logger2.setLevel(loglevel)\n",
    "        logger2.handler_set = True\n",
    "        \n",
    "    return logger\n",
    "\n",
    "def create_directory(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "\n",
    "\n",
    "import urllib.response\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import logging\n",
    "import requests, zipfile, io, sys, time\n",
    "#http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/2003/Qtr1/log20030101.zip\n",
    "     \n",
    "logger = get_logger()\n",
    "logger.info(\"Starting Program\")\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 0)\n",
    "time.sleep(1)\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 1)\n",
    "time.sleep(1)\n",
    "\n",
    "year_read = \"\"\n",
    "year = 0\n",
    "\n",
    "with open(\"config.txt\") as configfile:\n",
    "    for line in configfile:\n",
    "        name, val = line.partition(\"=\")[::2]\n",
    "        if (name.strip()==\"year\"):\n",
    "            year_read = val\n",
    "year = year_read.strip()\n",
    "\n",
    "try:\n",
    "    year = int(year)\n",
    "    if(year>=2003 and year<=2016):    \n",
    "        sys.stdout.write(\"\\rProgress : %d%%\" % 10)\n",
    "        time.sleep(1)\n",
    "\n",
    "        \n",
    "\n",
    "        sys.stdout.write(\"\\rProgress : %d%%\" % 10)\n",
    "        time.sleep(1)\n",
    "        #Data available only till: Qtr1,2016\n",
    "        url = []\n",
    "\n",
    "        if year == \"2016\":\n",
    "            qtr1_url(year)\n",
    "        #Any other year excep for 2016, all four quarters    \n",
    "        else:\n",
    "            logger.info(\"Generating Links for log files\")\n",
    "            sys.stdout.write(\"\\rProgress : %d%%\" % 15)\n",
    "            qtr1_url(year)\n",
    "            sys.stdout.write(\"\\rProgress : %d%%\" % 20)\n",
    "            qtr2_url(year)\n",
    "            sys.stdout.write(\"\\rProgress : %d%%\" % 25)\n",
    "            qtr3_url(year)\n",
    "            sys.stdout.write(\"\\rProgress : %d%%\" % 30)\n",
    "            qtr4_url(year)\n",
    "\n",
    "        #print(url[5])\n",
    "        #Extract all files into folder\n",
    "        i = 30\n",
    "        logger.info(\"Downloading and Extracting Log files\")\n",
    "        print(\"\\nIt may take some time\")\n",
    "        for link in url:\n",
    "            r = requests.get(link)\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall(path=\"Files2/\"+str(year))\n",
    "            i+=5\n",
    "            sys.stdout.write(\"\\rProgress : %d%%\" % i)\n",
    "        logger.info(\"Downloading Completed\") \n",
    "        logger.removeHandler(\"handler\")\n",
    "        logging.shutdown()\n",
    "        sys.stdout.write(\"\\rProgress : %d%%\" % 100)\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"Not a valid year\")\n",
    "        logger.removeHandler(\"handler\")\n",
    "        logging.shutdown()\n",
    "        sys.stdout.write(\"\\rProgress : %d%%\" % 100)\n",
    "        sys.stdout.flush()\n",
    "except:\n",
    "    print(\"exception Not a valid year\")\n",
    "    logger.removeHandler(\"handler\")\n",
    "    logging.shutdown()\n",
    "    sys.stdout.write(\"\\rProgress : %d%%\" % 100)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def split_date(fileData):\n",
    "    splitter = fileData['date'].apply(lambda x: x.split('-'))\n",
    "    fileData['year'] = splitter.apply(lambda x: x[0])\n",
    "    fileData['month'] = splitter.apply(lambda x: x[1])\n",
    "    fileData['dayOfMonth'] = splitter.apply(lambda x: x[2])\n",
    "    return fileData\n",
    "    \n",
    "\n",
    "def clean_size(fileData):\n",
    "    if (any(fileData['code'] == 304)):\n",
    "        fileData['size'].fillna(0, inplace=True)\n",
    "    return fileData\n",
    "\n",
    "def split_time(fileData):\n",
    "    splitter = fileData['time'].apply(lambda x: x.split(':'))\n",
    "    fileData['h'] = splitter.apply(lambda x: x[0])\n",
    "    fileData['m'] = splitter.apply(lambda x: x[1])\n",
    "    fileData['s'] = splitter.apply(lambda x: x[2])\n",
    "    return fileData\n",
    "\n",
    "def create_directory(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "def remove_code(fileData):\n",
    "    fileData = fileData[(fileData['code'] != 0)]\n",
    "    return fileData\n",
    "\n",
    "def processed_csvs(fileData, filePath,i):\n",
    "    fp = re.split('/', filePath, flags=re.IGNORECASE)\n",
    "    new_file_path = fp[0] + \"/\" + fp[1]  + \"/\"\n",
    "    create_directory(new_file_path + \"Cleaned_Files\")\n",
    "    new_file_path += \"Cleaned_Files/Cleaned_\" + fp[2] \n",
    "    fileData.to_csv(new_file_path, sep=',')\n",
    "#     if(i==1):\n",
    "#         fileData.to_csv(new_file_path, sep=',')\n",
    "#     else:\n",
    "#         with open(new_file_path, 'a') as f:\n",
    "#             fileData.to_csv(f, header=False)\n",
    "#     i=i+1\n",
    "\n",
    "\n",
    "def true_extention(fileData):\n",
    "    splitter = fileData['extention'].apply(lambda x: x.split('.'))\n",
    "    try:\n",
    "        fileData['file_name'] = splitter.apply(lambda x: x[0])\n",
    "        fileData['true_extention'] = splitter.apply(lambda x: x[1])\n",
    "#         no true_extention created\n",
    "    except:\n",
    "        pass\n",
    "    return fileData\n",
    "\n",
    "def handling_leftovers(fileData):\n",
    "    fileData['file_name'].replace(r'$^', np.nan, regex=True, inplace = True)\n",
    "    fileData.fillna(999999, inplace=True)\n",
    "#     print(\"_______________________________\")\n",
    "#     print(fileData['file_name'])\n",
    "    \n",
    "    return fileData\n",
    "\n",
    "def get_logger():\n",
    "    create_directory(\"Files2\")\n",
    "    loglevel = logging.INFO            # DEBUG, CRITICAL, WARNING, ERROR\n",
    "    logger = logging.getLogger(\"Application_Logs\")\n",
    "    logger2 = logging.getLogger(\"Application_Logs_Stream\")\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger.setLevel(logging.INFO)\n",
    "#         Logfile handler\n",
    "        handler = logging.FileHandler('Files2/logs.log')\n",
    "        handler2 = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.addHandler(handler2)\n",
    "        logger.setLevel(loglevel)\n",
    "        logger.handler_set = True\n",
    "#       Stream Handler\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger2.setLevel(logging.INFO)\n",
    "        handler2 = logging.StreamHandler()\n",
    "        handler2.setFormatter(formatter)\n",
    "        logger2.addHandler(handler2)\n",
    "        logger2.setLevel(loglevel)\n",
    "        logger2.handler_set = True\n",
    "        \n",
    "    return logger\n",
    "\n",
    "\n",
    "def create_directory(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "\n",
    "\n",
    "import os, sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging, sys, time\n",
    "\n",
    "logger = get_logger()\n",
    "logger.info(\"Starting Cleaning of Data\")\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 0)\n",
    "time.sleep(1)\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 1)\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "dir_path = \"Files2\"\n",
    "ls_dir = os.listdir(dir_path)\n",
    "year = 0;\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 3)\n",
    "# Finding Directory or year for which csv are present\n",
    "\n",
    "for file in ls_dir:\n",
    "    regexp = re.compile(r'.txt|.log')\n",
    "#     print(file)\n",
    "    if not(regexp.search(file)):\n",
    "        year = file\n",
    "\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 5)\n",
    "# Setting Directory for the year    \n",
    "if not(year == 0):\n",
    "#     print(year)\n",
    "    dir_path += \"/\" + str(year)\n",
    "else:\n",
    "    print(\"No Files found! Ending Program\")\n",
    "    sys.exit()\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 7)    \n",
    "#Looping over all the csv to load and process the data\n",
    "ls_dir = os.listdir(dir_path)\n",
    "\n",
    "i=1;\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 10)\n",
    "x = 10\n",
    "# print(ls_dir)\n",
    "for file in ls_dir:\n",
    "#     only if file is csv\n",
    "    logger.info(\"Cleaning \" + file)\n",
    "    regexp = re.compile(r'.csv')\n",
    "    if(regexp.search(file)):\n",
    "        filePath = dir_path + \"/\" + file\n",
    "    #   Reading File data with pandas\n",
    "        fileData = pd.read_csv(filePath,header = 0)\n",
    "\n",
    "        \n",
    "#       adding columns for year, month and day of month\n",
    "        fileData = split_date(fileData)\n",
    "#       adding columns for hour, minutes and seconds\n",
    "        fileData = split_time(fileData)\n",
    "#       adding column for true extension\n",
    "        fileData = true_extention(fileData)\n",
    "#       replace empty \"size\" with 0 if code equals 304\n",
    "        fileData = clean_size(fileData) \n",
    "#       removinf code with value 0\n",
    "        fileData = remove_code(fileData)\n",
    "#     replacing all other NaNs with 99999 \n",
    "        fileData = handling_leftovers(fileData)\n",
    "#         print(fileData.shape)\n",
    "#      processed data  \n",
    "        processed_csvs(fileData, filePath,i)\n",
    "        i=i+1;\n",
    "        x+=6\n",
    "        sys.stdout.write(\"\\rProgress : %d%%\" % x)\n",
    "print(\"\\nData Cleaned and Saved\")\n",
    "logger.removeHandler(\"handler\")\n",
    "logging.shutdown()\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 100)\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "# Saving dataframe to csv\n",
    "def saveToCsv(dataframe, filePath, summaryParam):\n",
    "    fp = re.split('/', filePath, flags=re.IGNORECASE)\n",
    "    directory = fp[0] + \"/\" + fp[1]  + \"/\" + fp [2] + \"/\"\n",
    "    directory += \"Summary\"\n",
    "    create_directory(directory)\n",
    "    new_file_path = directory + \"/\" + summaryParam + \"_\" + fp[3]\n",
    "    dataframe.to_csv(new_file_path, sep=',')\n",
    "\n",
    "def summary_count_unique_accn_per_month_func(fileData, filePath):\n",
    "#     Grouping by month and accession to get a list of unique month and accen pair\n",
    "    if not fileData.empty:\n",
    "        d= pd.DataFrame({'count' : fileData.groupby( [ \"month\", \"accession\"] ).size()}).reset_index()\n",
    "    # Grouping by month to get the count of unique accn numbers in the month ()\n",
    "        d= pd.DataFrame({'unique_accessions_accessed' : d.groupby( [ \"month\"] ).size()}).reset_index()\n",
    "        try:\n",
    "            saveToCsv(d, filePath, \"Count_Unique_Accession_Number\")\n",
    "    #         print(\"Unique accn Summary File Saved\")\n",
    "        except:\n",
    "            print(\"Unique accn summary file couldn't be saved. Permission Denied Exception\")\n",
    "            pass\n",
    "\n",
    "def get_logger():\n",
    "    create_directory(\"Files2\")\n",
    "    loglevel = logging.INFO            # DEBUG, CRITICAL, WARNING, ERROR\n",
    "    logger = logging.getLogger(\"Application_Logs\")\n",
    "    logger2 = logging.getLogger(\"Application_Logs_Stream\")\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger.setLevel(logging.INFO)\n",
    "#         Logfile handler\n",
    "        handler = logging.FileHandler('Files2/logs.log')\n",
    "        handler2 = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.addHandler(handler2)\n",
    "        logger.setLevel(loglevel)\n",
    "        logger.handler_set = True\n",
    "#       Stream Handler\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger2.setLevel(logging.INFO)\n",
    "        handler2 = logging.StreamHandler()\n",
    "        handler2.setFormatter(formatter)\n",
    "        logger2.addHandler(handler2)\n",
    "        logger2.setLevel(loglevel)\n",
    "        logger2.handler_set = True\n",
    "        \n",
    "    return logger\n",
    "\n",
    "\n",
    "def create_directory(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "def count_of_accession_for_code(fileData, filePath):\n",
    "\n",
    "    d = fileData.groupby(['month','code'])['accession'].count().reset_index(name=\"Count per Response\")\n",
    "#     print(type(d))\n",
    "    try:\n",
    "        saveToCsv(d, filePath, \"Count_Accession_For_Error_Code\")\n",
    "#         print(\"Count_Error_Codes_For_Accession Summary File Saved\")\n",
    "    except:\n",
    "        print(\"Count_Error_Codes_For_Accession summary file couldn't be saved. Permission Denied Exception\")\n",
    "        pass\n",
    "\n",
    "def count_of_files_by_extention(fileData, filePath):\n",
    "\n",
    "    d = fileData.groupby(['month','true_extention'])['true_extention'].count().reset_index(name=\"Count per Extension\")\n",
    "#     print(type(d))\n",
    "    try:\n",
    "        saveToCsv(d, filePath, \"Count_of_files_by_extention\")\n",
    "#         print(\"Count_Error_Codes_For_Accession Summary File Saved\")\n",
    "    except:\n",
    "        print(\"Count_of_files_by_extention summary file couldn't be saved. Permission Denied Exception\")\n",
    "        pass\n",
    "\n",
    "def summary_count_all_accn_for_month_func(fileData, filePath):\n",
    "    d= fileData.groupby('month')['accession'].count().reset_index(name=\"# of accessions\")\n",
    "    try:\n",
    "        saveToCsv(d, filePath, \"Count_All_Accession_Number\")\n",
    "\n",
    "    except:\n",
    "        print(\"Unique accn summary file couldn't be saved. Permission Denied Exception\")\n",
    "        pass\n",
    "\n",
    "import os, sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging, sys, time\n",
    "\n",
    "\n",
    "logger = get_logger()\n",
    "logger.info(\"Summarizing Data\")\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 0)\n",
    "time.sleep(1)\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 1)\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "dir_path = \"Files2\"\n",
    "ls_dir = os.listdir(dir_path)\n",
    "year = 0;\n",
    "\n",
    "# Finding Directory or year for which csv are present (this will look for the last folder only)\n",
    "for file in ls_dir:\n",
    "    regexp = re.compile(r'.txt|.log')\n",
    "#     print(file)\n",
    "    if not(regexp.search(file)):\n",
    "        year = file\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 4)    \n",
    "# Setting Directory for the year    \n",
    "if not(year == 0):\n",
    "#     print(year)\n",
    "    dir_path += \"/\" + str(year)\n",
    "else:\n",
    "    print(\"No Files found! Ending Program\")\n",
    "    sys.exit()\n",
    "    \n",
    "#Going into the Cleaned Files Directory\n",
    "\n",
    "\n",
    "\n",
    "dir_path += \"/Cleaned_Files/\"\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 24)\n",
    "ls_dir = os.listdir(dir_path)\n",
    "# print(ls_dir)\n",
    "\n",
    "# Looping over all cleaned csv files and summarizing the data\n",
    "x = 24\n",
    "\n",
    "for file in ls_dir:\n",
    "    \n",
    "    regexp = re.compile(r'.csv')\n",
    "    x += 6\n",
    "    \n",
    "#     Looking only for csv files in the directory\n",
    "    if(regexp.search(file)):\n",
    "        logger.info(\"Summarizing for \" + file)\n",
    "        filePath = dir_path + file\n",
    "        fileData = pd.read_csv(filePath,header = 0)\n",
    "        fileData = fileData.ix[:, 2:24]\n",
    "        sys.stdout.write(\"\\rProgress : %d%%\" % x)\n",
    "#     ____________________________________________________\n",
    "# CODE TO SUMMARIZE THE DATA\n",
    "\n",
    "#     number of unique accn numbers - accesd per month\n",
    "        summary_count_unique_accn_per_month_func(fileData, filePath)\n",
    "        count_of_accession_for_code(fileData, filePath)\n",
    "        summary_count_all_accn_for_month_func(fileData, filePath)\n",
    "        count_of_files_by_extention(fileData, filePath)\n",
    "        \n",
    "\n",
    "\n",
    "# _______________________________________________________________\n",
    "\n",
    "logger.info(\"Data Summarized and Saved\")\n",
    "logger.removeHandler(\"handler\")\n",
    "logging.shutdown()\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 100)\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    create_directory(\"Files2\")\n",
    "    loglevel = logging.INFO            # DEBUG, CRITICAL, WARNING, ERROR\n",
    "    logger = logging.getLogger(\"Application_Logs\")\n",
    "    logger2 = logging.getLogger(\"Application_Logs_Stream\")\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger.setLevel(logging.INFO)\n",
    "#         Logfile handler\n",
    "        handler = logging.FileHandler('Files2/logs.log')\n",
    "        handler2 = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.addHandler(handler2)\n",
    "        logger.setLevel(loglevel)\n",
    "        logger.handler_set = True\n",
    "#       Stream Handler\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger2.setLevel(logging.INFO)\n",
    "        handler2 = logging.StreamHandler()\n",
    "        handler2.setFormatter(formatter)\n",
    "        logger2.addHandler(handler2)\n",
    "        logger2.setLevel(loglevel)\n",
    "        logger2.handler_set = True\n",
    "        \n",
    "    return logger\n",
    "\n",
    "\n",
    "def create_directory(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "def zip_folder(folder_path, output_path):\n",
    "    \"\"\"Zip the contents of an entire folder (with that folder included\n",
    "    in the archive). Empty subfolders will be included in the archive\n",
    "    as well.\n",
    "    \"\"\"\n",
    "    parent_folder = os.path.dirname(folder_path)\n",
    "    # Retrieve the paths of the folder contents.\n",
    "    contents = os.walk(folder_path)\n",
    "\n",
    "    zip_file = zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED)\n",
    "    for root, folders, files in contents:\n",
    "        # Include all subfolders, including empty ones.\n",
    "        for folder_name in folders:\n",
    "            absolute_path = os.path.join(root, folder_name)\n",
    "            relative_path = absolute_path.replace(parent_folder + '\\\\', '')\n",
    "            zip_file.write(absolute_path, relative_path)\n",
    "        for file_name in files:\n",
    "            absolute_path = os.path.join(root, file_name)\n",
    "            relative_path = absolute_path.replace(parent_folder + '\\\\', '')\n",
    "            zip_file.write(absolute_path, relative_path)\n",
    "            \n",
    "\n",
    "import os, sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging, sys, time\n",
    "import zipfile\n",
    "import boto\n",
    "import boto.s3\n",
    "from boto.s3.key import Key\n",
    "\n",
    "logger = get_logger()\n",
    "logger.info(\"Compiling Data\")\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 0)\n",
    "time.sleep(1)\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 1)\n",
    "time.sleep(1)\n",
    "new_dir = \"Problem2_Compiled\"\n",
    "create_directory(new_dir)\n",
    "\n",
    "dir_path = \"Files2\"\n",
    "ls_dir = os.listdir(dir_path)\n",
    "\n",
    "\n",
    "year=0 \n",
    "\n",
    "aws_access_read = \"\"\n",
    "aws_secret_read = \"\"\n",
    "\n",
    "with open(\"config.txt\") as configfile:\n",
    "    for line in configfile:\n",
    "        name, val = line.partition(\"=\")[::2]\n",
    "        if (name.strip()==\"aws_access_key\"):\n",
    "            aws_access_read = val\n",
    "        elif (name.strip()==\"aws_secret_key\"):\n",
    "            aws_secret_read = val\n",
    "            \n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 10)\n",
    "time.sleep(1)\n",
    "\n",
    "aws_access_key = aws_access_read.strip()\n",
    "aws_secret_key = aws_secret_read.strip()\n",
    "\n",
    "\n",
    "if not (aws_access_key == \"\" or aws_access_read == \"\"):\n",
    "    # Finding Directory or year for which csv are present (this will look for the last folder only)\n",
    "    for file in ls_dir:\n",
    "        regexp = re.compile(r'.txt|.log')\n",
    "    #     print(file)\n",
    "        if not(regexp.search(file)):\n",
    "            year = file\n",
    "    sys.stdout.write(\"\\rProgress : %d%%\" % 4)    \n",
    "    # Setting Directory for the year    \n",
    "    if not(year == 0):\n",
    "    #     print(year)\n",
    "        dir_path += \"/\" + str(year)\n",
    "    else:\n",
    "        print(\"No Files found! Ending Program\")\n",
    "        sys.exit()\n",
    "\n",
    "    #Going into the Cleaned Files Directory    \n",
    "\n",
    "    # Compiling Cleaned Files\n",
    "    dir_path += \"/Cleaned_Files\"\n",
    "    sys.stdout.write(\"\\rProgress : %d%%\" % 7)\n",
    "    ls_dir = os.listdir(dir_path)\n",
    "\n",
    "    x = 7\n",
    "    for file in ls_dir:\n",
    "        x += 3\n",
    "        regexp = re.compile(r'Cleaned')\n",
    "        if(regexp.search(file)):\n",
    "            filePath = dir_path + \"/\" + file\n",
    "            fileData = pd.read_csv(filePath,header = 0)    \n",
    "            compiled_file_path = new_dir + \"/Cleaned.csv\"\n",
    "            if(os.path.exists(compiled_file_path)):\n",
    "                with open(compiled_file_path , 'a') as f:\n",
    "                    fileData.to_csv(f, header=False)\n",
    "            else:\n",
    "                fileData.to_csv(compiled_file_path, header=True)\n",
    "        sys.stdout.write(\"\\rProgress : %d%%\" % x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ____________for loop ends_____________________________\n",
    "    # Compiling Cleaned Files Ends    \n",
    "\n",
    "\n",
    "\n",
    "        #Going into the Summary Files Directory\n",
    "    # Compiling Summary Files\n",
    "    dir_path += \"/Summary\"\n",
    "    sys.stdout.write(\"\\rProgress : %d%%\" % 24)\n",
    "    ls_dir = os.listdir(dir_path)\n",
    "\n",
    "    for file in ls_dir:\n",
    "        x += 1\n",
    "        regexp = re.compile(r'Count_Accession_For_Error_Code')\n",
    "        if(regexp.search(file)):\n",
    "            filePath = dir_path + \"/\" + file\n",
    "            fileData = pd.read_csv(filePath,header = 0)    \n",
    "            compiled_file_path = new_dir + \"/Count_Accession_For_Error_Code.csv\"\n",
    "            if(os.path.exists(compiled_file_path)):\n",
    "                with open(compiled_file_path , 'a') as f:\n",
    "                    fileData.to_csv(f, header=False)\n",
    "            else:\n",
    "                fileData.to_csv(compiled_file_path, header=True)\n",
    "\n",
    "\n",
    "        regexp = re.compile(r'Count_Unique_Accession_Number_Cleaned')\n",
    "        if(regexp.search(file)):\n",
    "            filePath = dir_path + \"/\" + file\n",
    "            fileData = pd.read_csv(filePath,header = 0)    \n",
    "            compiled_file_path = new_dir + \"/Count_Unique_Accession_Number_Cleaned.csv\"\n",
    "            if(os.path.exists(compiled_file_path)):\n",
    "                with open(compiled_file_path , 'a') as f:\n",
    "                    fileData.to_csv(f, header=False)\n",
    "            else:\n",
    "                fileData.to_csv(compiled_file_path, header=True)\n",
    "\n",
    "\n",
    "\n",
    "        regexp = re.compile(r'Count_All_Accession_Number')\n",
    "        if(regexp.search(file)):\n",
    "            filePath = dir_path + \"/\" + file\n",
    "            fileData = pd.read_csv(filePath,header = 0)    \n",
    "            compiled_file_path = new_dir + \"/Count_All_Accession_Number.csv\"\n",
    "            if(os.path.exists(compiled_file_path)):\n",
    "                with open(compiled_file_path , 'a') as f:\n",
    "                    fileData.to_csv(f, header=False)\n",
    "            else:\n",
    "                fileData.to_csv(compiled_file_path, header=True)\n",
    "\n",
    "\n",
    "        regexp = re.compile(r'Count_of_files_by_extention')\n",
    "        if(regexp.search(file)):\n",
    "            filePath = dir_path + \"/\" + file\n",
    "            fileData = pd.read_csv(filePath,header = 0)    \n",
    "            compiled_file_path = new_dir + \"/Count_of_files_by_extention.csv\"\n",
    "            if(os.path.exists(compiled_file_path)):\n",
    "                with open(compiled_file_path , 'a') as f:\n",
    "                    fileData.to_csv(f, header=False)\n",
    "            else:\n",
    "                fileData.to_csv(compiled_file_path, header=True)\n",
    "        sys.stdout.write(\"\\rProgress : %d%%\" % x)\n",
    "\n",
    "    # ____________for loop ends_____________________________\n",
    "    # Compiling Summary Files Ends\n",
    "\n",
    "\n",
    "    zip_folder('Problem2_Compiled', 'Problem2_Compiled.zip')\n",
    "    logger.info(\"Problem2_Compiled Files Zipped\")\n",
    "\n",
    "\n",
    "    logger.info(\"Uploading to Amazon s3\")\n",
    "     #Uploading Files to S3\n",
    "\n",
    "    bucket_name = aws_access_key.lower() \n",
    "    conn = boto.connect_s3(aws_access_key,aws_secret_key)\n",
    "\n",
    "    #Checking if bucket exists\n",
    "    bucket = conn.lookup(bucket_name)\n",
    "    if bucket is None:\n",
    "        bucket = conn.create_bucket(bucket_name, location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "    testfile = \"Problem2_Compiled.zip\"\n",
    "    def percent_cb(complete, total):\n",
    "        sys.stdout.write('.')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    k = Key(bucket)\n",
    "    k.key = testfile\n",
    "    k.set_contents_from_filename(testfile,\n",
    "    cb=percent_cb, num_cb=10)\n",
    "    logger.info(\"Files Uploaded to S3\")\n",
    "\n",
    "    logger.info(\"Compiled and uploaded to Amazon s3\")\n",
    "    logger.removeHandler(\"handler\")\n",
    "    logging.shutdown()\n",
    "    sys.stdout.write(\"\\rProgress : %d%%\" % 100)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "else:\n",
    "    print(\"Please check your config file for aws keys\")\n",
    "    logger.removeHandler(\"handler\")\n",
    "    logging.shutdown()\n",
    "    sys.stdout.write(\"\\rProgress : %d%%\" % 100)\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
