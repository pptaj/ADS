{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cik_read = 1652044accn_read = 0001652044-17-000008\n",
      "274\n",
      "383\n",
      "Program Ended\n"
     ]
    }
   ],
   "source": [
    "#10 K = 0001652044-17-000008, \n",
    "#1652044\n",
    "\n",
    "import time\n",
    "import urllib.response\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "#html = urlopen(\"https://www.sec.gov/Archives/edgar/data/1652044/000165204417000008/goog10-kq42016.htm\")\n",
    "    #html = urlopen(\"https://www.sec.gov/Archives/edgar/data/51143/000005114313000007/ibm13q3_10q.htm\")\n",
    "\n",
    "def create_csv10k(cik,document_accession_number, url):\n",
    "    try:\n",
    "        \n",
    "        html = urlopen(url)\n",
    "        soup = BeautifulSoup(html,\"lxml\")\n",
    "        all_tables = soup.find_all(\"table\")\n",
    "        type(all_tables)\n",
    "\n",
    "        tables = []\n",
    "        req_tables = []\n",
    "        for x in all_tables:\n",
    "            tables.append(x)\n",
    "\n",
    "        for table in tables:\n",
    "            all_rows =  table.find_all('tr')\n",
    "            for row in all_rows:\n",
    "                cols = row.find_all('td')\n",
    "                for col in cols:\n",
    "                    if col.text.find('$') > -1 or col.text.find('%') > -1:\n",
    "                        req_tables.append(table)\n",
    "                        break\n",
    "\n",
    "        print(len(req_tables))\n",
    "        print(len(tables))        \n",
    "\n",
    "        i=0\n",
    "        for table in req_tables:\n",
    "            data = []\n",
    "            i= i + 1\n",
    "            all_rows =  table.find_all('tr')\n",
    "            for row in all_rows:\n",
    "                cols = row.find_all('td')\n",
    "                cols = [(''.join(ch.strip('[\\n,$]') for ch in ele.text)).strip() for ele in cols]\n",
    "                #print(cols)\n",
    "                data.append([ele for ele in cols if ele])\n",
    "            create_directory(\"Files\")\n",
    "            create_directory(\"Files/\" +cik)\n",
    "            create_directory(\"Files/\" + cik + \"/\" +document_accession_number)\n",
    "            dir = \"Files/\" + cik + \"/\" + document_accession_number\n",
    "            write_file = open( dir+\"/table_\" + str(i) + '.csv', 'w')\n",
    "            \n",
    "            for row in data:\n",
    "                for column in row:\n",
    "                    write_file.write(column)\n",
    "                    write_file.write(',')\n",
    "                write_file.write('\\n')\n",
    "  \n",
    "    except:\n",
    "        print(\"Something went wrong with the 10-K Page\")\n",
    "        pass\n",
    "\n",
    "    #intitalize row and column numbers\n",
    "\n",
    "def create_csv_10Q(cik,document_accession_number, url):\n",
    "    try:        \n",
    "        html = urlopen(url)\n",
    "        soup = BeautifulSoup(html,\"lxml\")\n",
    "        all_tables = soup.find_all(\"table\", border=1)\n",
    "        type(all_tables)\n",
    "        tables = []\n",
    "        for x in all_tables:\n",
    "            tables.append(x)\n",
    "        i=0\n",
    "\n",
    "        for table in tables:\n",
    "            data = []\n",
    "            i= i + 1\n",
    "            all_rows =  table.find_all('tr')\n",
    "            for row in all_rows:\n",
    "                cols = row.find_all('td')\n",
    "                cols = [(''.join(ch.strip('[\\n,$]') for ch in ele.text)).strip() for ele in cols]\n",
    "                #print(cols)\n",
    "                data.append([ele for ele in cols if ele])\n",
    "            create_directory(\"Files\")\n",
    "            create_directory(\"Files/\" +cik)\n",
    "            create_directory(\"Files/\" + cik + \"/\" +document_accession_number)\n",
    "            dir = \"Files/\" + cik + \"/\" + document_accession_number\n",
    "            write_file = open( dir+\"/table_\" + str(i) + '.csv', 'w')\n",
    "            for row in data:\n",
    "                for column in row:\n",
    "                    write_file.write(column)\n",
    "                    write_file.write(',')\n",
    "                write_file.write('\\n')\n",
    "\n",
    "    except:\n",
    "        print(\"Something went wrong with the 10-Q Page\")\n",
    "        pass\n",
    "        #intitalize row and column numbers\n",
    "\n",
    "def create_directory(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    \n",
    "# READ CIK, ACCESSION NUMBER AND AMAZON KEYS FROM config.txt\n",
    "cik_read =\"\"\n",
    "accn_num_read =\"\"\n",
    "aws_read = \"\"\n",
    "#aws_secret_access_key = ''\n",
    "\n",
    "#bucket_name = AWS_ACCESS_KEY_ID.lower() + '-dump'\n",
    "#conn = boto.connect_s3(aws_access_key_id, aws_secret_access_key)\n",
    "\n",
    "with open(\"config.txt\") as configfile:\n",
    "    for line in configfile:\n",
    "        name, val = line.partition(\"=\")[::2]\n",
    "        if (name==\"cik\"):\n",
    "            cik_read = val\n",
    "        elif (name==\"accession_number\"):\n",
    "            accn_num_read = val\n",
    "        elif (name==\"aws_read\"):\n",
    "            aws_read = val\n",
    "        #elif (name==\"AWS_SECRET_ACCESS_KEY\"):\n",
    "            #aws_secret_access_key = val\n",
    "#bucket = conn.create_bucket(bucket_name, location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "print(\"cik_read = \"+ cik_read.strip()+ \"accn_read = \" + accn_num_read.strip())\n",
    "\n",
    "# cik = input(\"Please enter CIK: \")\n",
    "# document_accession_number = input(\"Please enter accession number: \")\n",
    "cik = cik_read.strip()\n",
    "document_accession_number = accn_num_read.strip()\n",
    "aws_key = aws_read.strip()\n",
    "\n",
    "#GENERATING THE URL\n",
    "document_accession_number_without_dashes = re.sub('[-]', '', document_accession_number)\n",
    "url = \"https://www.sec.gov/Archives/edgar/data/\" + str(int(cik)) + \"/\" + document_accession_number_without_dashes + \"/\"\n",
    "url = url + document_accession_number +\"-index.html\"\n",
    "\n",
    "try:\n",
    "    \n",
    "#     GENERATING URL TO GET 10-q or 10-k filing\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    cells = soup.findAll('td')\n",
    "\n",
    "    filing_flag = \"NA\"\n",
    "    s = \"https://www.sec.gov\"\n",
    "    for cell in cells :\n",
    "        if cell.get_text().find('10-Q') > -1 :\n",
    "            filing_flag = \"10-Q\"\n",
    "            a = cell.find_next_sibling(\"td\").find('a',href=True)\n",
    "            if a is not None:\n",
    "                if 'href' in a.attrs:\n",
    "                    result = a['href']\n",
    "                    s+=result\n",
    "                    break\n",
    "        \n",
    "                    \n",
    "    \n",
    "    for cell in cells :\n",
    "        if cell.get_text().find('10-K') > -1 :\n",
    "            filing_flag = \"10-K\"\n",
    "            a = cell.find_previous_sibling(\"td\").find('a')\n",
    "            if a is not None:\n",
    "                if 'href' in a.attrs:\n",
    "                    result = a['href']\n",
    "                    s+=result\n",
    "                    break\n",
    "\n",
    "                    \n",
    "# Calling Function to go in 10-Q or 10-K filing page and getting the table data into csv \n",
    "    if(filing_flag == \"10-Q\"):\n",
    "        create_csv_10Q(cik,document_accession_number, s)\n",
    "    elif(filing_flag == \"10-K\"):\n",
    "        create_csv10k(cik,document_accession_number, s) \n",
    "    else:\n",
    "        print(\"No 10-K or 10-Q filing found for given CIK and Accession number\")\n",
    "    print(\"Program Ended\")\n",
    "    \n",
    "except:\n",
    "    print(\"The cik and accession number pair doesn't exist\")\n",
    "    print(\"Program Ended With Error\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
