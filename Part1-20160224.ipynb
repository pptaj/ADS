{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress : 2%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Data From Config File\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress : 10%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your CIK IS 1652044 and Accession Number is 0001652044-17-000008\n",
      "GENERATING THE URL\n",
      "GENERATING URL TO GET 10-q or 10-k filing\n",
      "10-K FILING FOUND FOR YOUR CIK AND ACCESSION NUMBER PAIR\n",
      "10-K FILING FOUND FOR YOUR CIK AND ACCESSION NUMBER PAIR\n",
      "Downloading 10-K Filing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 35%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PARSING ALL TABLES in 10-K filing\n",
      "SAVING TABLES TO .csv file\n",
      ".csv Files Created\n",
      "Files Zipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Uploaded to S3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 99%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Program Ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress : 100%"
     ]
    }
   ],
   "source": [
    "#10 K = 0001652044-17-000008, \n",
    "#1652044\n",
    "\n",
    "import time\n",
    "import urllib.response\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import string\n",
    "from string import punctuation\n",
    "import zipfile\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import boto\n",
    "import boto.s3\n",
    "from boto.s3.key import Key\n",
    "import zipfile\n",
    "\n",
    "def get_logger():\n",
    "    create_directory(\"Files\")\n",
    "    loglevel = logging.INFO            # DEBUG, CRITICAL, WARNING, ERROR\n",
    "    logger = logging.getLogger(\"Application_Logs\")\n",
    "    logger2 = logging.getLogger(\"Application_Logs_Stream\")\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger.setLevel(logging.INFO)\n",
    "#         Logfile handler\n",
    "        handler = logging.FileHandler('Files/logs.log')\n",
    "        handler2 = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.addHandler(handler2)\n",
    "        logger.setLevel(loglevel)\n",
    "        logger.handler_set = True\n",
    "#       Stream Handler\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "        logger2.setLevel(logging.INFO)\n",
    "        handler2 = logging.StreamHandler()\n",
    "        handler2.setFormatter(formatter)\n",
    "        logger2.addHandler(handler2)\n",
    "        logger2.setLevel(loglevel)\n",
    "        logger2.handler_set = True\n",
    "        \n",
    "    return logger\n",
    "#html = urlopen(\"https://www.sec.gov/Archives/edgar/data/1652044/000165204417000008/goog10-kq42016.htm\")\n",
    "    #html = urlopen(\"https://www.sec.gov/Archives/edgar/data/51143/000005114313000007/ibm13q3_10q.htm\")\n",
    "\n",
    "def zip_folder(folder_path, output_path):\n",
    "    \"\"\"Zip the contents of an entire folder (with that folder included\n",
    "    in the archive). Empty subfolders will be included in the archive\n",
    "    as well.\n",
    "    \"\"\"\n",
    "    parent_folder = os.path.dirname(folder_path)\n",
    "    # Retrieve the paths of the folder contents.\n",
    "    contents = os.walk(folder_path)\n",
    "\n",
    "    zip_file = zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED)\n",
    "    for root, folders, files in contents:\n",
    "        # Include all subfolders, including empty ones.\n",
    "        for folder_name in folders:\n",
    "            absolute_path = os.path.join(root, folder_name)\n",
    "            relative_path = absolute_path.replace(parent_folder + '\\\\', '')\n",
    "            zip_file.write(absolute_path, relative_path)\n",
    "        for file_name in files:\n",
    "            absolute_path = os.path.join(root, file_name)\n",
    "            relative_path = absolute_path.replace(parent_folder + '\\\\', '')\n",
    "            zip_file.write(absolute_path, relative_path)\n",
    "            break\n",
    "    \n",
    "def create_csv10k(cik,document_accession_number,url,aws_access_key,aws_secret_key):\n",
    "    try:\n",
    "        logger = get_logger()\n",
    "        logger.info(\"Downloading 10-K Filing\")\n",
    "        html = urlopen(url)\n",
    "        soup = BeautifulSoup(html,\"lxml\")\n",
    "        logger.info(\"PARSING ALL TABLES in 10-K filing\")\n",
    "        all_tables = soup.find_all(\"table\")\n",
    "        type(all_tables)\n",
    "\n",
    "        tables = []\n",
    "        req_tables = []\n",
    "        for x in all_tables:\n",
    "            tables.append(x)\n",
    "\n",
    "        for table in tables:\n",
    "            all_rows =  table.find_all('tr')\n",
    "            for row in all_rows:\n",
    "                cols = row.find_all('td')\n",
    "                for col in cols:\n",
    "                    if col.text.find('$') > -1 or col.text.find('%') > -1:\n",
    "                        req_tables.append(table)\n",
    "                        break\n",
    "        \n",
    "#         print(len(req_tables))\n",
    "#         print(len(tables))        \n",
    "\n",
    "        i=0\n",
    "        logger.info(\"SAVING TABLES TO .csv file\")\n",
    "        for table in req_tables:\n",
    "            data = []\n",
    "            i= i + 1\n",
    "            all_rows =  table.find_all('tr')\n",
    "            \n",
    "            for row in all_rows:\n",
    "                cols = row.find_all('td')\n",
    "                cols = [(''.join(ch.strip('[\\n,$]') for ch in ele.text)).strip() for ele in cols]\n",
    "                #print(cols)\n",
    "                data.append([ele for ele in cols if ele])\n",
    "            create_directory(\"Files\")\n",
    "            create_directory(\"Files/\" +cik)\n",
    "            create_directory(\"Files/\" + cik + \"/\" +document_accession_number)\n",
    "            dir = \"Files/\" + cik + \"/\" + document_accession_number\n",
    "            write_file = open( dir+\"/table_\" + str(i) + '.csv', 'w')\n",
    "            \n",
    "            \n",
    "            for row in data:\n",
    "                for column in row:\n",
    "                    write_file.write(column)\n",
    "                    write_file.write(',')\n",
    "                write_file.write('\\n')\n",
    "            write_file.close()\n",
    "        logger.info(\".csv Files Created\")\n",
    "        \n",
    "        #Zipping files\n",
    "        zip_folder('Files', 'Files.zip')\n",
    "        logger.info(\"Files Zipped\")\n",
    "        \n",
    "        #Uploading Files to S3\n",
    "        \n",
    "        bucket_name = aws_access_key.lower() \n",
    "        conn = boto.connect_s3(aws_access_key,aws_secret_key)\n",
    "\n",
    "        bucket = conn.create_bucket(bucket_name, location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "        testfile = \"Files.zip\"\n",
    "        def percent_cb(complete, total):\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        k = Key(bucket)\n",
    "        k.key = testfile\n",
    "        k.set_contents_from_filename(testfile,\n",
    "        cb=percent_cb, num_cb=10)\n",
    "        logger.info(\"Files Uploaded to S3\")\n",
    "        \n",
    "    except:\n",
    "        print(\"Something went wrong with the 10-K Page\")\n",
    "        print(sys.exc_info())\n",
    "        pass\n",
    "\n",
    "    #intitalize row and column numbers\n",
    "    \n",
    "def create_directory(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        \n",
    "def create_csv_10Q(cik,document_accession_number,url,aws_access_key,aws_secret_key):\n",
    "    try:\n",
    "        logger = get_logger()\n",
    "        logger.info(\"Downloading 10-Q Filing\")\n",
    "        html = urlopen(url)\n",
    "        soup = BeautifulSoup(html,\"lxml\")\n",
    "        logger.info(\"PARSING ALL TABLES in 10-Q filing\")\n",
    "        all_tables = soup.find_all(\"table\", border=1)\n",
    "        type(all_tables)\n",
    "        tables = []\n",
    "        for x in all_tables:\n",
    "            tables.append(x)\n",
    "        i=0\n",
    "        logger.info(\"SAVING TABLES TO .csv file\")\n",
    "        for table in tables:\n",
    "            data = []\n",
    "            i= i + 1\n",
    "            all_rows =  table.find_all('tr')\n",
    "            \n",
    "            for row in all_rows:\n",
    "                cols = row.find_all('td')\n",
    "                cols = [(''.join(ch.strip('[\\n,$]') for ch in ele.text)).strip() for ele in cols]\n",
    "                data.append([ele for ele in cols if ele])\n",
    "            \n",
    "            create_directory(\"Files\")\n",
    "            create_directory(\"Files/\" +cik)\n",
    "            create_directory(\"Files/\" + cik + \"/\" +document_accession_number)\n",
    "            dir = \"Files/\" + cik + \"/\" + document_accession_number\n",
    "            write_file = open( dir+\"/table_\" + str(i) + '.csv', 'w')\n",
    "            \n",
    "            for row in data:\n",
    "                for column in row:\n",
    "                    write_file.write(column)\n",
    "                    write_file.write(',')\n",
    "                write_file.write('\\n')\n",
    "            write_file.close()   \n",
    "        logger.info(\".csv FILES CREATED\")\n",
    "        \n",
    "         #Zipping files\n",
    "        zip_folder('Files', 'Files.zip')\n",
    "        logger.info(\"Files Zipped\")\n",
    "        \n",
    "        #Uploading Files to S3\n",
    "        \n",
    "        bucket_name = aws_access_key.lower() \n",
    "        conn = boto.connect_s3(aws_access_key,aws_secret_key)\n",
    "\n",
    "        bucket = conn.create_bucket(bucket_name, location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "        testfile = \"Files.zip\"\n",
    "        def percent_cb(complete, total):\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        k = Key(bucket)\n",
    "        k.key = testfile\n",
    "        k.set_contents_from_filename(testfile,\n",
    "        cb=percent_cb, num_cb=10)\n",
    "        logger.info(\"Files Uploaded to S3\")\n",
    "        \n",
    "    except:\n",
    "        logger.error(\"Something went wrong with the 10-Q Page\")\n",
    "        pass\n",
    "        #intitalize row and column numbers\n",
    "        \n",
    "# time.sleep(1)\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 0)\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 2)\n",
    "time.sleep(1)\n",
    "# Adding logger\n",
    "logger = get_logger()\n",
    "logger.info(\"Reading Data From Config File\")\n",
    "\n",
    "\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 5)    \n",
    "time.sleep(1)\n",
    "# READ CIK, ACCESSION NUMBER AND AMAZON KEYS FROM config.txt\n",
    "cik_read =\"\"\n",
    "accn_num_read =\"\"\n",
    "aws_access_read = \"\"\n",
    "aws_secret_read = \"\"\n",
    "\n",
    "with open(\"config.txt\") as configfile:\n",
    "    for line in configfile:\n",
    "        name, val = line.partition(\"=\")[::2]\n",
    "        if (name==\"cik\"):\n",
    "            cik_read = val\n",
    "        elif (name==\"accession_number\"):\n",
    "            accn_num_read = val\n",
    "        elif (name==\"aws_access_key\"):\n",
    "            aws_access_read = val\n",
    "        elif (name==\"aws_secret_key\"):\n",
    "            aws_secret_read = val\n",
    "            \n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 10)\n",
    "time.sleep(1)\n",
    "\n",
    "logger.info(\"Your CIK IS \"+ cik_read.strip()+ \" and Accession Number is \" + accn_num_read.strip())\n",
    "\n",
    "# cik = input(\"Please enter CIK: \")\n",
    "# document_accession_number = input(\"Please enter accession number: \")\n",
    "cik = cik_read.strip()\n",
    "document_accession_number = accn_num_read.strip()\n",
    "aws_access_key = aws_access_read.strip()\n",
    "aws_secret_key = aws_secret_read.strip()\n",
    "\n",
    "#GENERATING THE URL\n",
    "logger.info(\"GENERATING THE URL\")\n",
    "document_accession_number_without_dashes = re.sub('[-]', '', document_accession_number)\n",
    "url = \"https://www.sec.gov/Archives/edgar/data/\" + str(int(cik)) + \"/\" + document_accession_number_without_dashes + \"/\"\n",
    "url = url + document_accession_number +\"-index.html\"\n",
    "sys.stdout.write(\"\\rProgress : %d%%\" % 35)\n",
    "try:\n",
    "    \n",
    "#     GENERATING URL TO GET 10-q or 10-k filing\n",
    "    logger.info(\"GENERATING URL TO GET 10-q or 10-k filing\")\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    cells = soup.findAll('td')\n",
    "\n",
    "    filing_flag = \"NA\"\n",
    "    s = \"https://www.sec.gov\"\n",
    "    for cell in cells :\n",
    "        if cell.get_text().find('10-Q') > -1 :\n",
    "            filing_flag = \"10-Q\"\n",
    "            logger.info(\"10-Q FILING FOUND FOR YOUR CIK AND ACCESSION NUMBER PAIR\")\n",
    "            a = cell.find_next_sibling(\"td\").find('a',href=True)\n",
    "            if a is not None:\n",
    "                if 'href' in a.attrs:\n",
    "                    result = a['href']\n",
    "                    s+=result\n",
    "                    break               \n",
    "    \n",
    "    for cell in cells :\n",
    "        if cell.get_text().find('10-K') > -1 :\n",
    "            filing_flag = \"10-K\"\n",
    "            logger.info(\"10-K FILING FOUND FOR YOUR CIK AND ACCESSION NUMBER PAIR\")\n",
    "            a = cell.find_previous_sibling(\"td\").find('a')\n",
    "            if a is not None:\n",
    "                if 'href' in a.attrs:\n",
    "                    result = a['href']\n",
    "                    s+=result\n",
    "                    break\n",
    "# sys.stdout.write(\"\\rProgress : %d%%\" % 70)\n",
    "                    \n",
    "# Calling Function to go in 10-Q or 10-K filing page and getting the table data into csv \n",
    "    if(filing_flag == \"10-Q\"):\n",
    "        create_csv_10Q(cik,document_accession_number, s, aws_access_key, aws_secret_key)\n",
    "        sys.stdout.write(\"\\rProgress : %d%%\" % 99)\n",
    "        time.sleep(1)\n",
    "    elif(filing_flag == \"10-K\"):\n",
    "        create_csv10k(cik,document_accession_number, s, aws_access_key, aws_secret_key)\n",
    "        sys.stdout.write(\"\\rProgress : %d%%\" % 99)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        logger.error(\"No 10-K or 10-Q filing found for given CIK and Accession number\")\n",
    "    logger.info(\"Program Ended\")\n",
    "    \n",
    "except:\n",
    "    logger.error(\"The cik and accession number pair doesn't exist\")\n",
    "    logger.info(\"Program Ended With Error\")\n",
    "    pass\n",
    "finally: \n",
    "#     Removing Handler, Shutting Down Logger and Ending Progress Bar\n",
    "    logger.removeHandler(\"handler\")\n",
    "    logging.shutdown()\n",
    "    sys.stdout.write(\"\\rProgress : %d%%\" % 100)\n",
    "    \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
